\documentclass[11pt]{article}

% Guidelines
% Abstract (short)
% Biography (~30 words for each author)   
% Keywords (up to six): 
% 2000 and 5000
% No limit on the number of figures

\usepackage[margin=1in]{geometry}
%\usepackage{endfloat}

<<load-packages-options,cache=FALSE,echo=FALSE,message=FALSE,warning=FALSE>>=
library(ggplot2)
library(gdata)
library(reshape2)
library(plyr)
#Set some knitr options
opts_knit$set(progress = TRUE, verbose = TRUE)
opts_chunk$set(cache=FALSE, fig.width=16, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE)
@




\begin{document}

\title{Comparability and reproducibility of biomedical data}

\author{Raphael Gottardo and Yunda Yuang}

\date{\today}
\maketitle



% Outline: 
% 1.  There is an increasing need for comparability and reproducibility (C\&R) of biomedical data
% 2.	Overview of data generation process to identify each step that may influence C\&R
% 3.	Identify metrics to quantify C&R and their relation to accuracy and precision – recommend practices that increase C&R and recommend gold standard and positive/negative controls for meaningful research 
% 4.	Recommend analysis methods to factors that contribute to comparability and reproducibility 
% In the U.S. alone, biomedical research is a \$100-billion-year enterprise. This has stimulated an increasing availability of biomedical data, including those generated by traditional analytical technologies, as well as those from modern high-throughput “omics” platforms. Such a great amount of information provides researchers unprecedented opportunities to investigate scientific questions that otherwise could not be answered.  However, it also poses imperative concerns from the research community regarding the comparability and reproducibility of biomedical data. Can data generated from one source or laboratory be compared or pooled with those from another? Can data generated from one laboratory be reproduced in another? And how much do data-processing procedures play a role in such investigations? Many studies have been conducted to address one or multiple of these questions. In this report, we will systematically review various sources that contribute to the comparability and reproducibility of data, make recommendations on experiment designs that can increase comparability and reproducibility of data, and review analytical methods that address such an issue. For a related topic on reproducibility of study results, we direct readers to [ref: 1, 2, 3]. 
% 
% Let’s first look at a prototypical biomedical data generation process. As shown in Figure 1, the process of how information contained in a biological sample is quantified into numeric values for statistical /bioinformatical analysis can be roughly broken down into four steps.  In step 1 where biological samples are prepared to be measured, there are several factors that may influence the comparability and reproducibility of data, including common factors such as the specific type of measuring system (ref: xx), and the standard operating procedure (ref: xx) for biological sample preparation, experiment layout, and measurement, as well as other experimental conditions that are often not specified in the protocol. For example, xx (list studies that investigated effect of technician and time etc.). Therefore, in Step 1, to increase comparability and reproducibility of data, all these factors should be thought out and optimally controlled and standardized if possible. When factors such as technician and time are not feasible to be standardized across multiple labs, a measuring system should strive to minimize variations from these factors.  In step 2, raw information from an instrument is calibrated and quantified into numeric values. Xxx. Therefore, in step 2, standardized algorithms for these operations are recommended and, if possible, they are also implemented using the same software.  In Step 3, xxx 

\begin{abstract}
\end{abstract}

\section{Introduction}

% Some text to motivate the reproducibility of data analysis
Over the past two decades, the biomedical field has been transformed by the advent of new high throughput technologies including gene expression microarrays, protein arrays, flow cytometry and next generation sequencing. Because these technologies generate large high dimensional data from a single machine run, data management and analysis have become an integral part of any scientific experiment. 
In addition to the experimental protocol, data analysis contributes significantly to the reproducibility or non-reproducibility of an experiment or publication. Unfortunately, as of today, too many published studies remain irreproducible due to the lack of sharing of data and/or computer code or scripts that can be used to reproduce the analysis. This lack of reproducibility has even gone as far as to stop a cancer clinical trial based on gene expression signatures that could not be reproduced by independent researchers. Should have the data and computer code been made available, the results of the study could have been invalidated more rapidly, which could have saved funding and avoided giving patients false hope. Fortunately, over the past decade computers, software tools and online resources have drastically improved to the point that it is easier than ever to share data, code and construct fully reproducible data analysis pipelines. 

In this paper we review some of the fundamental issues involved in the reproducibility and comparability of biomedical data going from assay standardization to reproducible data analysis. This paper is not meant to be an exhaustive review of all possible assays and problems but rather to select a few concrete examples and present some thoughts and solutions towards the overall C\&R concept. 
The paper is organized as follows 

\section{Reproducibility of assay and primary data}
\subsection{Overview of data generation process and its impact on C\&R}
Let`s first look at a prototypical biomedical data generation process. As shown in Figure 1, it can be roughly broken down into three core steps of information transfomation from signals contained in a biological sample to numeric values captured in datasets for analysis. In step 1, biological samples are measured and raw instrument data are generated, There are several factors that may influence the C\&R of data in this step. These include some of the more obvious factors such as the specific type of technologies (e.g., hybridization-based or sequence-based gene expression) or platforms (e.g., Affymetrix, Illumina or Operon) (e.g., ref: Larkin et al, 2005), the standard operating procedures for biological sample preparation, experiment layout, and measurement(e.g., ref: Ach et al., 2007), as well as other experimental conditions that are often not specified in the experiment protocol. For example, the exprience level or learning effect of the experiment technicians (ref: Todd et al., 2012), origins of the experiment reagents (ref: ) or batch effect of the reagents (ref: ) are all possible sources for differences beteween experimental results. Therefore, in Step 1, to increase the C\&R of data, all these factors should be thought out and optimally controlled and standardized if possible. When factors such as technicians or reagent batches are not feasible to be standardized across multiple studies or labs, a measuring system comprised of a specific platform using a specific technology should strive to minimize variations from these factors and increase robustness against changes in these factors.  In step 2, raw information from an instrument is calibrated and quantified into numeric values. Because this step often involves image analyses for information alignment or dimension reduction, the specific algorithms used to make such transformation, the implementation of such algorithms and the specific data storing structure including data format and naming conventions for variables will all be vital to keep consistent and standardized for the C\&R of the resulting data. Lastly, in Step 3, data resulted from step 2 are further processed before any meaningful study objective-driven analyses are conducted. This step often involves further data alignment such as background-adjustment or data aggregation such as per-biomarker summarization from multiple subset measurements. Certain quality assurance and control processing may also occur to remove unreliable data and reduce any systematic variations between data points. Similarly as in Step 2, the specification and implementation of the algorithm, and the data storing sturcture are important to be noted in the effort to pertain C\& R of the resulting data. In section 3, we will discuss some of the tools available to share data resulted from step 2 and associated computer code for data processing and analysis.       

\subsection{Metrics to quantify C\&R}
Being more concretely defined, accuracy and precision can be used as two building-block metrics to illustrate the comparability and reproducibility of data while the definitions of the later concepts may vary between context.  Accuracy indicates how close a measurement is to its true (actual) value, whereas precision indicates how close measurements are to each other. Deviation of accuracy (i.e., bias) is often introduced by systematic sources. For example, factors mentioned earlier such as the measuring system may be a primary source of bias that cannot be removed by repeating measurements or averaging large number of results. On the other hand, precision (i.e., variability) of data can generally be improved by increasing the number of measurements. This is often one of the reasons that biological replicates and technical replicates are recommended in experiment design to capture inherent variations from biological samples and from the experiment conduct, respectively. 

Interestingly, as demonstrated in Figure 2, comparable and reproducible data do not necessarily require unbiased measurements as long as they are “consistently inaccurate”. On the other hand, accurate and precise measurements from multiple sources sufficiently lead to comparable and reproducible data. In another word, a gold standard for the measured quantities and the consistence of measurements to that gold standard are not necessarily required to establish comparability and reproducibility.  Therefore, to ensure meaningful integrative analysis of biomedical data from multiple sources, we encourage the inclusion of gold standard whenever possible, or established positive and negative controls in the experiments when otherwise.    

\begin{figure}
\begin{center}
<<"data-for-reproducibility", fig.width=8,fig.height=6,dev=c('pdf', 'postscript'),echo=FALSE,dev.args=list(pointsize=16)>>=
# Set a seed for reproducibility
set.seed(6)

# True estimate
true.mean<-6
# Number of replicates
n<-100
biased.low.var<-rnorm(n,true.mean+3,sd=1)
biased.high.var<-rnorm(n,true.mean+2,sd=1.5)
unbiased.low.var<-rnorm(n,true.mean,sd=2)
unbiased.high.var<-rnorm(n,true.mean+3,sd=3)

data<-data.frame(Replicates=c(unbiased.low.var,unbiased.high.var,biased.low.var,biased.high.var),variance=c(rep("Low variance",n),rep("High variance",n),rep("Low variance",n),rep("High variance",n)),bias=c(rep("Unbiased",n),rep("Unbiased",n),rep("Biased",n),rep("Biased",n)),Protocol=c(rep("A",n),rep("B",n),rep("C",n),rep("D",n)),Exp=rep("x",4*n))

ggplot()+geom_boxplot(data=data,aes(y=Replicates,x=Exp,fill=Protocol,alpha=Protocol),outlier.size=0)+geom_abline(data=data,intercept=true.mean,slope=0,size=2,color="black",alpha=.5)+facet_grid(variance~bias)+theme_bw()+opts(panel.grid.major=theme_blank(),panel.grid.minor=theme_blank(),axis.text.y = theme_blank(),axis.text.x = theme_blank(), axis.title.x = theme_blank(), axis.ticks=theme_blank())+ annotate("text", 0, 6, label = "Truth", hjust = -0.5, vjust = -0.5)
@
\caption{Variance-Bias trade off. }

\end{center}
\end{figure}

\subsection{Correcting for experimental bias}
% Here we will talk about preprocessing such as normalization, batch effect correction, etc. 
% Talk about preprocessing for variance-bias (e.g. Microarrays and background subtraction)
When there is possible experiment-specific bias in data for comparison or pooling, various data processing steps can be taken to align the data to the same scale (i.e., normalziation) or to take away batch effect based on certain assumptions of the data. xxx

when these assumptions are not satisfied in lower-dimension biomedical data, internal or external validation data are usually used to correct for measurement error that may be related to measurement, instrument or sampling design. When there is lack of standards for a quantity’s true value (e.g., Horton et al. 2007), calibration methods based on paired-sample (e.g., Huang et al. 2012) can be adopted to adjust for experiment bias. 


\subsection{Standards and data sharing}
Although the process of achieving ultimate C\&R of biomedical data will have to take incremental steps, setting up standards for assay data generation process, data management and data analyses are necessary efforts towards such a goal. For example, the National Institutes of Health (NIH) announced plans in 2009 for the development of data sharing policies involing ``sequence and related genomic data obtained with advanced sequencing technology (e.g., medical resequencing data, sequence data from non-human species, including microorganisms, transcriptomic and epigenomic data, as well as data needed for interpretation, including associated clinical, other phenotype and metadata, such as supporting study documents and methodologies)''. Private donors such as the Bill and Melinda Gates Fundation,the Wellcome Trust and the Hewlett Foundation  Much of the infrastructures, technical standards, and incentives that are needed to support data sharing are lacking, and these data can hold particular sensitivities. And some researchers are reluctant to share data. Too often, data are treated as the private property of investigators who aim to maximise their publication record at the expense of the widest possible use of the data. This situation threatens to limit both the progress of this research and its application for public health benefit.

    2. Encourage investigators and IRBs to consider the potential for broad sharing of sequence and related genomic data in developing informed consent processes and documents for such studies involving human sequence data; and,
    3. Communicate the agency’s intent and current underlying considerations related to developing a policy pertaining to the deposition of these large datasets into centralized databases, such as the GenBank Short Read Archive (SRA) or the Database of Genotypes and Phenotypes (dbGaP), so that they are available as broadly and rapidly as possible to a wide range of scientific investigators.CAVD, HIPC, Cancer immunotherapy etc.

\section{Reproducibility of assay results and derived data}

Here we discuss some of the tools available to researchers to perform reproducible analysis and share processed data, computer code and final results. 

\subsection{Tools for reproducible analyses}
% R and Bioconductor
% RStudio
% BioPerl, BioPython
% GenomeSpace
% LabKey
\subsection{Standards and code sharing}
In the same fashion that experimental protocols need to be published in order for an experiment to be reproduced, computer code, software and data should also be published along with the results of a data analysis. Ideally, software would be open source and computer code would be well package and standardize to facilitate exchange and usability. 
% GitHub
% RStudio
% Open Source
\subsection{Authoring tools}
% Sweave, knitr, Rstudio, openoffice, pandoc
% GenePattern and Word plugin


Several tools have been proposed to automatically incorporate 
reproducible data analysis pipelines or computer code into documents.
An example is the GenePattern Word plugin that can be used to embed analysis pipelines in a document and rerun them on any GenePattern server from the Word application. 
Another example that is popular among statisticians and bioinformatics is the Sweave literate language that allows one to create dynamic reports by embedding R code in latex documents. This is also our preferred approach because it is open source and does not depend on propriety software. In addition, recent software development such as RStudio and knitr made working with Sweave even more accessible, which should reduce the learning curve for most users. In fact, this article was written using the Sweave language and processed using RStudio. 
Ideally, all material including the Sweave source file, computer code and data, which Gentleman refers to as a compendium, would be made available along with the final version of the manuscript and be open access, allowing anyone to reproduce the results or identify potential problems in the analysis. This openness should further improve the impact of open access papers and journals over non-open access journals by giving more credibility to the published results. Unfortunately, currently very few journals are pushing for reproducibility and even less have clear reproducibility policies. An example of a journal moving in the right direction is Biostatistics, for which one of us is associate editor. Biostatistics now has a reproducility guideline and is now working with authors towards making sure that published results are reproducible given that data and code are provided (as described in the guileline). When data and code are provided, and results are reproducible, the article is marked with an R. 

\subsection{Open science}

\section{Conclusion}


\end{document}