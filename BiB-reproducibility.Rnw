\documentclass[11pt]{article}

% Guidelines
% Abstract (short)
% Biography (~30 words for each author)   
% Keywords (up to six): 
% 2000 and 5000
% No limit on the number of figures

\usepackage[margin=1in]{geometry}
%\usepackage{endfloat}

<<load-packages-options,cache=FALSE,echo=FALSE,message=FALSE,warning=FALSE>>=
library(ggplot2)
library(gdata)
library(reshape2)
library(plyr)
#Set some knitr options
opts_knit$set(progress = TRUE, verbose = TRUE)
opts_chunk$set(cache=FALSE, fig.width=16, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE)
@




\begin{document}

\title{Comparability and reproducibility of biomedical data}

\author{Raphael Gottardo and Yunda Yuang}

\date{\today}
\maketitle



% Outline: 
% 1.  There is an increasing need for comparability and reproducibility (C\&R) of biomedical data
% 2.	Overview of data generation process to identify each step that may influence C\&R
% 3.	Identify metrics to quantify C&R and their relation to accuracy and precision – recommend practices that increase C&R and recommend gold standard and positive/negative controls for meaningful research 
% 4.	Recommend analysis methods to factors that contribute to comparability and reproducibility 
% In the U.S. alone, biomedical research is a \$100-billion-year enterprise. This has stimulated an increasing availability of biomedical data, including those generated by traditional analytical technologies, as well as those from modern high-throughput “omics” platforms. Such a great amount of information provides researchers unprecedented opportunities to investigate scientific questions that otherwise could not be answered.  However, it also poses imperative concerns from the research community regarding the comparability and reproducibility of biomedical data. Can data generated from one source or laboratory be compared or pooled with those from another? Can data generated from one laboratory be reproduced in another? And how much do data-processing procedures play a role in such investigations? Many studies have been conducted to address one or multiple of these questions. In this report, we will systematically review various sources that contribute to the comparability and reproducibility of data, make recommendations on experiment designs that can increase comparability and reproducibility of data, and review analytical methods that address such an issue. For a related topic on reproducibility of study results, we direct readers to [ref: 1, 2, 3]. 
% 
% Let’s first look at a prototypical biomedical data generation process. As shown in Figure 1, the process of how information contained in a biological sample is quantified into numeric values for statistical /bioinformatical analysis can be roughly broken down into four steps.  In step 1 where biological samples are prepared to be measured, there are several factors that may influence the comparability and reproducibility of data, including common factors such as the specific type of measuring system (ref: xx), and the standard operating procedure (ref: xx) for biological sample preparation, experiment layout, and measurement, as well as other experimental conditions that are often not specified in the protocol. For example, xx (list studies that investigated effect of technician and time etc.). Therefore, in Step 1, to increase comparability and reproducibility of data, all these factors should be thought out and optimally controlled and standardized if possible. When factors such as technician and time are not feasible to be standardized across multiple labs, a measuring system should strive to minimize variations from these factors.  In step 2, raw information from an instrument is calibrated and quantified into numeric values. Xxx. Therefore, in step 2, standardized algorithms for these operations are recommended and, if possible, they are also implemented using the same software.  In Step 3, xxx 

\begin{abstract}
\end{abstract}

\section{Introduction}

% Some text to motivate the reproducibility of data analysis
Over the past two decades, the biomedical field has been transformed by the advent of new high throughput technologies including gene expression microarrays, protein arrays, flow cytometry and next generation sequencing. Because these technologies generate large high dimensional data from a single machine run, data management and analysis have become an integral part of any scientific experiment. 
In addition to the experimental protocol, data analysis contributes significantly to the reproducibility or non-reproducibility of an experiment or publication. Unfortunately, as of today, too many published studies remain irreproducible due to the lack of sharing of data and/or computer code or scripts that can be used to reproduce the analysis. This lack of reproducibility has even gone as far as to stop a cancer clinical trial based on gene expression signatures that could not be reproduced by independent researchers. Should have the data and computer code been made available, the results of the study could have been invalidated more rapidly, which could have saved funding and avoided giving patients false hope. Fortunately, over the past decade computers, software tools and online resources have drastically improved to the point that it is easier than ever to share data, code and construct fully reproducible data analysis pipelines. 

In this paper we review some of the fundamental issues involved in the reproducibility and comparability of biomedical data going from assay standardization to reproducible data analysis. This paper is not meant to be an exhaustive review of all possible assays and problems but rather to select a few concrete examples and present some thoughts and solutions towards the overall C\&R concept. 
The paper is organized as follows 

\section{Reproducibility of assay and primary data}
\subsection{Overview of data generation process and its impact on C\&R}
Let's first use a prototypical biomedical data generation process to illustrate factors associated with different stages of this process that may impact the C\&R of data. As shown in Figure 1, a data generation process can be roughly broken down into three core stages of information transfomation from signals contained in a biological sample to numeric values captured in datasets for analysis. In Stage 1, biological samples are measured and raw instrument data are generated. There are several factors that may influence the C\&R of data in this stage. These include some of the more obvious factors such as the specific type of technologies (e.g., hybridization-based or sequence-based gene expression) or platforms (e.g., Affymetrix, Illumina or Operon) (e.g., ref: Larkin et al, 2005), the standard operating procedures for biological sample preparation, experiment layout, and measurement(e.g., ref: Ach et al., 2007), as well as other conditions that are often not specified in the experiment protocol. For example, the exprience level or learning effect of the experiment technicians (ref: Todd et al., 2012), or the origin or batch effect of the experiment reagents (ref: ) are also possible sources for differences beteween experimental results. Therefore, in Stage 1, to increase the C\&R of data, all these factors should be thought out and optimally controlled and standardized whenever possible. When factors such as technicians or reagent batches may not be feasible to standardize across multiple studies or labs, a measuring system comprised of a specific platform using a specific technology should strive to minimize variations from these factors and increase robustness against changes in these factors.  In Stage 2, raw information from an instrument is calibrated and quantified into numeric values. Because this stage often involves image analyses for information alignment and\/or dimension reduction, the specific algorithms used to make such transformation, the implementation of such algorithms and the specific data storing structure including data format and naming conventions for variables are also vital to keep consistent and standardized to a maximal level for the C\&R of the resulting data. Lastly, in Stage 3, data resulted from Stage 2 are further processed before study objective-driven analyses are conducted. This stage often involves further data alignment such as background-adjustment or data aggregation such as per-biomarker summarization from multiple subset measurements. Certain quality assurance and control processing may also occur to remove unreliable data and reduce any systematic variations between data points. Similarly as in Stage 2, the specification and implementation of the algorithms, and the data storing sturcture are important to be noted in the effort to pertain C\& R of the resulting data. In section 3, we will discuss some of the tools available to share Stage 2 data and associated computer code for data processing and analysis.       

\subsection{Metrics to quantify C\&R}
Being more concretely defined, accuracy and precision can be used as two building-block metrics to illustrate the comparability and reproducibility of data while the definitions of the later concepts may vary between context.  Accuracy indicates how close a measurement is to its true (actual) value, whereas precision indicates how close measurements are to each other. Deviation of accuracy (i.e., bias) is often introduced by systematic sources. For example, factors mentioned earlier such as the measuring system may be a primary source of bias that cannot be removed by repeating measurements or averaging large number of results. On the other hand, precision (i.e., variability) of data can generally be improved by increasing the number of measurements. This is often one of the reasons that biological replicates and technical replicates are recommended in experiment design to capture inherent variations from biological samples and from the experiment conduct, respectively. 

Interestingly, as demonstrated in Figure 2, comparable and reproducible data do not necessarily require unbiased measurements as long as they are “consistently inaccurate”. On the other hand, accurate and precise measurements from multiple sources sufficiently lead to comparable and reproducible data. In another word, a gold standard for the measured quantities and the consistence of measurements to that gold standard are not necessarily required to establish comparability and reproducibility.  Therefore, to ensure meaningful integrative analysis of biomedical data from multiple sources, we encourage the inclusion of gold standard whenever possible, or established positive and negative controls in the experiments when otherwise.    

\begin{figure}
\begin{center}
<<"data-for-reproducibility", fig.width=8,fig.height=6,dev=c('pdf', 'postscript'),echo=FALSE,dev.args=list(pointsize=16)>>=
# Set a seed for reproducibility
set.seed(6)

# True estimate
true.mean<-6
# Number of replicates
n<-100
biased.low.var<-rnorm(n,true.mean+3,sd=1)
biased.high.var<-rnorm(n,true.mean+2,sd=1.5)
unbiased.low.var<-rnorm(n,true.mean,sd=2)
unbiased.high.var<-rnorm(n,true.mean+3,sd=3)

data<-data.frame(Replicates=c(unbiased.low.var,unbiased.high.var,biased.low.var,biased.high.var),variance=c(rep("Low variance",n),rep("High variance",n),rep("Low variance",n),rep("High variance",n)),bias=c(rep("Unbiased",n),rep("Unbiased",n),rep("Biased",n),rep("Biased",n)),Protocol=c(rep("A",n),rep("B",n),rep("C",n),rep("D",n)),Exp=rep("x",4*n))

ggplot()+geom_boxplot(data=data,aes(y=Replicates,x=Exp,fill=Protocol,alpha=Protocol),outlier.size=0)+geom_abline(data=data,intercept=true.mean,slope=0,size=2,color="black",alpha=.5)+facet_grid(variance~bias)+theme_bw()+opts(panel.grid.major=theme_blank(),panel.grid.minor=theme_blank(),axis.text.y = theme_blank(),axis.text.x = theme_blank(), axis.title.x = theme_blank(), axis.ticks=theme_blank())+ annotate("text", 0, 6, label = "Truth", hjust = -0.5, vjust = -0.5)
@
\caption{Variance-Bias trade off. }

\end{center}
\end{figure}

\subsection{Correcting for experimental bias}
% Here we will talk about preprocessing such as normalization, batch effect correction, etc. 
% Talk about preprocessing for variance-bias (e.g. Microarrays and background subtraction)
When there is possible experiment-specific bias in data for comparison or pooling, various data processing steps can be taken to align the data to the same scale (i.e., normalziation) or to take away batch effect based on certain assumptions of the data. xxx

when these assumptions are not satisfied in lower-dimension biomedical data, internal or external validation data are usually used to correct for measurement error that may be related to measurement, instrument or sampling design. When there is lack of standards for a quantity’s true value (e.g., Horton et al. 2007), calibration methods based on paired-sample (e.g., Huang et al. 2012) can be adopted to adjust for experiment bias. 


\subsection{Standards and data sharing}
Although the process of achieving ultimate C\&R of biomedical data will have to take incremental steps, setting up standards for assay data generation process, data management and data analyses are necessary efforts towards such a goal. As datasets get richer and richer with more data, more variables and more metadata, it is important to define standards that can be used to capture and distribute all necessary information that is necessary to achieve reproducibility. Several standards have been proposed for biomedical data that achieve these goals including MIAME for gene expression, MINSEQE for sequencing experiment or MinFlowCyt for flow cytometry. In addition to assay protocol information, it is important that any preprocessing done to the the data be fully described, \textit{e.g.} normalization for microarrays or gating for flow cytometry. Unfortunately, too many assays are still lacking data standards (e.g. bead array multiplex assays) or if data standards are available, manufacturers and/or software companies have been slow at adpoting them. For example, despite the availability of a data standards for defining transformation and gates for flow cytometry, no analysis software has yet fully adopted this format, and it is impossible to share reproducible analyses across software platforms. We basically had to write custorm open source software that can read some of the file formats output by major software companies. 

% Data sharing policy for organization
The National Institutes of Health (NIH) has been very supportive to the creation and adoption of standards for biomedical data. For example, we are part of the Human Immunology Project Consurtium, a project funded by the NIH, where much investments have been done for the creation of data standards for immunology data. When data have been standardized, it is important that data be shared for the benefit of science and to this extend funding agencies have yet an important role to play. In 2009, the NIH announced a plan for the development of data sharing policies involing ``sequence and related genomic data obtained with advanced sequencing technology (e.g., medical resequencing data, sequence data from non-human species, including microorganisms, transcriptomic and epigenomic data, as well as data needed for interpretation, including associated clinical, other phenotype and metadata, such as supporting study documents and methodologies)''. Private donors such as the Bill and Melinda Gates Fundation,the Wellcome Trust and the Hewlett Foundation  Much of the infrastructures, technical standards, and incentives that are needed to support data sharing are lacking, and these data can hold particular sensitivities. And some researchers are reluctant to share data. Too often, data are treated as the private property of investigators who aim to maximise their publication record at the expense of the widest possible use of the data. This situation threatens to limit both the progress of this research and its application for public health benefit.

    2. Encourage investigators and IRBs to consider the potential for broad sharing of sequence and related genomic data in developing informed consent processes and documents for such studies involving human sequence data; and,
    3. Communicate the agency’s intent and current underlying considerations related to developing a policy pertaining to the deposition of these large datasets into centralized databases, such as the GenBank Short Read Archive (SRA) or the Database of Genotypes and Phenotypes (dbGaP), so that they are available as broadly and rapidly as possible to a wide range of scientific investigators.CAVD, HIPC, Cancer immunotherapy etc.

% Data sharing policy for journals
Unfortunately, as mentioned in Alsheikh-Ali et al.~(2011), too few journal have clear policies for data deposition and even less make it mendatory for publication. Even when the policy makes it a requirement, the majority did not fully follow the data availability instructions.

\section{Reproducibility of assay results and derived data}

Here we discuss some of the tools available to researchers to perform reproducible analysis and share processed data, computer code and final results. Analysis of data issued from high throughput experiments can be extremely complex, involving multiple steps from data formating, pre-processing to statistical inference. Thus it is important that all steps be recorded for full reproducibility. Unfortunatelly, this is difficult to do with a point-and-click software interface, where there is no easy to save intermediate step. This is not to mention the fact that the ``manual'' analysis of a typical high throughput data set would typically require the used of multiple software and be very time consuming. In addition, it is not clear how robust the conclusions of a study are to small perturbations in any of these analysis steps. As such, it might be a good idea to be able to quickly redo an analysis after tuning some parameters to optimize the analysis; something that is not practical with a typical point-and-click software. 


\subsection{Tools for reproducible analyses}
In the recent years, several open source, community-based projects have emerged that enable researchers to contrusct and share complete and fully reproducible data analysis pipelines.
The Bioconductor project, based on the R statistical language, provide more than 500 software packages for the analysis of a wide range of biomedical data from gene expression microarrays to flow cytometry and next generation sequencing. These packages can be combined via a script written in the R language to form complex data analysis pipelines, connect to data reporistory, and generate high quality graphics. The resulting R script can then be used to record and later reproduce the analysis (along with all input parameters). Because all steps of the analysis are automated when the script is executed, it makes it easier to assess the robustness of the results when tuning some parameters. Other similar projects with perhaps more focused capabilities include Biopython and Bioperl that are based on the Python and Perl languages, respectively. For example, neither BioPython nor Perl have tools for the analysis of flow cytometry data. 

Even though several graphical user interfaces (e.g. RStudio for R) are available for writing computer scripts based on R/Bioconductor (or BioPerl, BioPython), the learning curve can still be steep for novice users. Several web based tools are now available to construct data analysis pipelines using a combinations of available modules that are for the most part wrappers for packages available in R, Perl or Python (or some other language). For example, a popular open source graphical-user-interface for gene expression analysis, GenePattern, versions every pipeline and its methods, ensuring that each version of a pipeline (and its results) remain static, even the pipeline continues to progress. A more recent project, GenomeSpace, funded by the National Human Genome Research Institute (NHGRI) can now combine GenePattern with other popular Bioinformatics tools including Galaxy, Cytoscape and the UCSC genome browser. As such, users can perform all of their analysis using a single platform. In the clinical and immunological field, LabKey is a popular OS tool for sharing immunological data and building complex analysis pipelines that can be shared with other users. LabKey is currently being used by large research networks including the CAVD, the HIV Vaccine Trial Network and the Immune Tolerance Network.

\subsection{Standards and code sharing}
In the same fashion that experimental protocols need to be published in order for an experiment to be reproduced, computer code, software and data should also be published along with the results of a data analysis. Ideally, software would be open source and computer code would be well package and standardize to facilitate exchange and usability. Both Bioconductor and GenePattern, mentioned above, provide facilities for users to package and share code with other users. Bioconductor is based on the R packaging system, which is highly standardized and has been a driving force behind R and Bioconductor gained in popularity. Bioconductor goes even further by 1) ensuring that all submitted packages are peer reviewed and 2) by providing version control repositories and built systems where source code is maintained, versioned and binaries automatically built for all operating systems. 
Among other things, the peer review process ensures that the package follows some basic guidelines, are well documented, work as advertised and are useful to the community. The open source and versioning system provide full access to algorithms and their implementation which are crucial to obtain full reproducibility. 
For users that want to version and share software code outside of the Bioconductor (or similar) project, there exist many free web-based hosting services to store, version and share code (and even data). One of our favorite platform is GitHub, which the componay markets as "Social Coding for all". GitHub makes it easy for anyone to store and version control computer code, packages, create documents, webpages and even wikis to document their code. The social aspect of GitHub makes it easy for users to work in teams on a common project. GitHub is free of all open source projects.  

Unfortunately, very few journals have code sharing/software policies and even less have requirements on the code/software being open access. For example, BMC Bioinformatics, for which one of us is associate editor, only has policies for Software articles, and even for these the source code is not required, only an executable. PLoS One requires to release software and make code open source for submission in which software is the central part of the paper. Although this policy is clearer, it is still up to the editor/reviewers to decide whether software was a central part. In a day and age where most experiments generate large amounts of data, software is always going to play a central part, so why not make this policy universal for all submission involving data analysis?
This being said, based on our own experience as associate editors, we feel that reviewers are pushing in the right direction by asking that code be open source and released along with the paper. So even if journal have no clear policies yet, we, the community, can enforce that code be released everytime we review a paper. 

\subsection{Authoring tools}
% Sweave, knitr, Rstudio, openoffice, pandoc
% GenePattern and Word plugin

Several tools have been proposed to automatically incorporate 
reproducible data analysis pipelines or computer code into documents.
An example is the GenePattern Word plugin that can be used to embed analysis pipelines in a document and rerun them on any GenePattern server from the Word application. 
Another example that is popular among statisticians and bioinformatics is the Sweave literate language that allows one to create dynamic reports by embedding R code in latex documents. This is our preferred approach because it is open source and does not depend on propriety software. In fact, every Bioconductor package is required to have a fully reproducible documentation (called vignette) written in the Sweave language. In addition, recent software development such as RStudio and knitr made working with Sweave even more accessible, which should reduce the learning curve for most users. In fact, this article was written using the Sweave language and processed using RStudio, and the source file (along with all versions of it) is available from GitHub.
Ideally, all material including the Sweave source file, computer code and data, which Gentleman refers to as a \textit{compendium}, would be made available along with the final version of the manuscript and be open access, allowing anyone to reproduce the results or identify potential problems in the analysis. An option would be to package, code, data and the Sweave source file into an R package for ease of distribution as is commonly done for Bioconductor data packages.
Journals that promote to this openness should further improve their impact versus non open journals by giving more credibility to the published results. Unfortunately, currently very few journals are pushing for reproducibility and even less have clear reproducibility policies. An example of a journal moving in the right direction is Biostatistics, for which one of us is associate editor. Biostatistics now has a reproducility guideline and is now working with authors towards making sure that published results are reproducible given that data and code are provided (as described in the guilelines). When data and code are provided, and results are reproducible, the article is marked with an R. 


\section{Conclusion}
% Talk about the importance of bioinformatics/biostatistics and LDO groups

\end{document}