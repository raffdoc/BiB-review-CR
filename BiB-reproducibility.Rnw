\documentclass[11pt]{article}

% Guidelines
% Abstract (short)
% Biography (~30 words for each author)   
% Keywords (up to six): 
% 2000 and 5000
% No limit on the number of figures

\usepackage[margin=1in]{geometry}
%\usepackage{endfloat}

<<load-packages-options,cache=FALSE,echo=FALSE,message=FALSE,warning=FALSE>>=
library(ggplot2)
library(gdata)
library(reshape2)
library(plyr)
#Set some knitr options
opts_knit$set(progress = TRUE, verbose = TRUE)
opts_chunk$set(cache=FALSE, fig.width=16, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE)
@




\begin{document}

\title{Comparability and reproducibility of biomedical data}

\author{Raphael Gottardo and Yunda Yuang}

\date{\today}
\maketitle



% Outline: 
% 1.  There is an increasing need for comparability and reproducibility (C\&R) of biomedical data
% 2.	Overview of data generation process to identify each step that may influence C\&R
% 3.	Identify metrics to quantify C&R and their relation to accuracy and precision – recommend practices that increase C&R and recommend gold standard and positive/negative controls for meaningful research 
% 4.	Recommend analysis methods to factors that contribute to comparability and reproducibility 
% In the U.S. alone, biomedical research is a \$100-billion-year enterprise. This has stimulated an increasing availability of biomedical data, including those generated by traditional analytical technologies, as well as those from modern high-throughput “omics” platforms. Such a great amount of information provides researchers unprecedented opportunities to investigate scientific questions that otherwise could not be answered.  However, it also poses imperative concerns from the research community regarding the comparability and reproducibility of biomedical data. Can data generated from one source or laboratory be compared or pooled with those from another? Can data generated from one laboratory be reproduced in another? And how much do data-processing procedures play a role in such investigations? Many studies have been conducted to address one or multiple of these questions. In this report, we will systematically review various sources that contribute to the comparability and reproducibility of data, make recommendations on experiment designs that can increase comparability and reproducibility of data, and review analytical methods that address such an issue. For a related topic on reproducibility of study results, we direct readers to [ref: 1, 2, 3]. 
% 
% Let’s first look at a prototypical biomedical data generation process. As shown in Figure 1, the process of how information contained in a biological sample is quantified into numeric values for statistical /bioinformatical analysis can be roughly broken down into four steps.  In step 1 where biological samples are prepared to be measured, there are several factors that may influence the comparability and reproducibility of data, including common factors such as the specific type of measuring system (ref: xx), and the standard operating procedure (ref: xx) for biological sample preparation, experiment layout, and measurement, as well as other experimental conditions that are often not specified in the protocol. For example, xx (list studies that investigated effect of technician and time etc.). Therefore, in Step 1, to increase comparability and reproducibility of data, all these factors should be thought out and optimally controlled and standardized if possible. When factors such as technician and time are not feasible to be standardized across multiple labs, a measuring system should strive to minimize variations from these factors.  In step 2, raw information from an instrument is calibrated and quantified into numeric values. Xxx. Therefore, in step 2, standardized algorithms for these operations are recommended and, if possible, they are also implemented using the same software.  In Step 3, xxx 

\begin{abstract}
\end{abstract}

\section{Introduction}

% Some text to motivate the reproducibility of data analysis
Over the past two decades, the biomedical field has been transformed by the advent of new high throughput technologies including gene expression microarrays, protein arrays, flow cytometry and next generation sequencing. Because these technologies generate large high dimensional data from a single machine run, data management and analysis have become an integral part of any scientific experiment. 
In addition to the experimental protocol, data analysis contributes significantly to the reproducibility or non-reproducibility of an experiment or publication. Unfortunately, as of today, too many published studies remain irreproducible due to the lack of sharing of data and/or computer code or scripts that can be used to reproduce the analysis. This lack of reproducibility has even gone as far as to stop a cancer clinical trial based on gene expression signatures that could not be reproduced by independent researchers. Should have the data and computer code been made available, the results of the study could have been invalidated more rapidly, which could have saved funding and avoided giving patients false hope. Fortunately, over the past decade computers, software tools and online resources have drastically improved to the point that it is easier than ever to share data, code and construct fully reproducible data analysis pipelines. 

In this paper we review some of the fundamental issues involved in the reproducibility and comparability of biomedical data going from assay standardization to reproducible data analysis. This paper is not meant to be an exhaustive review of all possible assays and problems but rather to select a few concrete examples and present some thoughts and solutions towards the overall C\&R concept. 
The paper is organized as follows 

\section{Reproducibility of assay and primary data}
\subsection{Overview of data generation process and its impact on C\&R}
Let’s first look at a prototypical biomedical data generation process. As shown in Figure 1, the process can be roughly broken down into four components. of how information contained in a biological sample is quantified into numeric values for data analysis  In step 1 where biological samples are prepared to be measured, there are several factors that may influence the comparability and reproducibility of data, including common factors such as the specific type of measuring system (ref: xx), and the standard operating procedure (ref: xx) for biological sample preparation, experiment layout, and measurement, as well as other experimental conditions that are often not specified in the protocol. For example, xx (list studies that investigated effect of technician and time etc.). Therefore, in Step 1, to increase comparability and reproducibility of data, all these factors should be thought out and optimally controlled and standardized if possible. When factors such as technician and time are not feasible to be standardized across multiple labs, a measuring system should strive to minimize variations from these factors.  In step 2, raw information from an instrument is calibrated and quantified into numeric values. Xxx. Therefore, in step 2, standardized algorithms for these operations are recommended and, if possible, they are also implemented using the same software.  In Step 3, xxx 

\subsection{Metrics to quantify C\&R}

\begin{figure}
\begin{center}
<<"data-for-reproducibility", fig.width=8,fig.height=6,dev=c('pdf', 'postscript'),echo=FALSE,dev.args=list(pointsize=16)>>=
# Set a seed for reproducibility
set.seed(6)

# True estimate
true.mean<-6
# Number of replicates
n<-100
biased.low.var<-rnorm(n,true.mean+3,sd=1)
biased.high.var<-rnorm(n,true.mean+2,sd=1.5)
unbiased.low.var<-rnorm(n,true.mean,sd=2)
unbiased.high.var<-rnorm(n,true.mean+3,sd=3)

data<-data.frame(Replicates=c(unbiased.low.var,unbiased.high.var,biased.low.var,biased.high.var),variance=c(rep("Low variance",n),rep("High variance",n),rep("Low variance",n),rep("High variance",n)),bias=c(rep("Unbiased",n),rep("Unbiased",n),rep("Biased",n),rep("Biased",n)),Protocol=c(rep("A",n),rep("B",n),rep("C",n),rep("D",n)),Exp=rep("x",4*n))

ggplot()+geom_boxplot(data=data,aes(y=Replicates,x=Exp,fill=Protocol,alpha=Protocol),outlier.size=0)+geom_abline(data=data,intercept=true.mean,slope=0,size=2,color="black",alpha=.5)+facet_grid(variance~bias)+theme_bw()+opts(panel.grid.major=theme_blank(),panel.grid.minor=theme_blank(),axis.text.y = theme_blank(),axis.text.x = theme_blank(), axis.title.x = theme_blank(), axis.ticks=theme_blank())+ annotate("text", 0, 6, label = "Truth", hjust = -0.5, vjust = -0.5)
@
\caption{Variance-Bias trade off. }

\end{center}
\end{figure}

\subsection{Correcting for experimental bias}
% Here we will talk about preprocessing such as normalization, batch effect correction, etc. 
% Talk about preprocessing for variance-bias (e.g. Microarrays and background subtraction)
\subsection{Standards and data sharing}
% talk about standards here
Unfortunately, as mentioned in Alsheikh-Ali et al.~(2011), too few journal have clear policies for data deposition and even less make it mendatory for publication. Even when the policy makes it a requirement, the majority did not fully follow the data availability instructions.

\section{Reproducibility of assay results and derived data}

Here we discuss some of the tools available to researchers to perform reproducible analysis and share processed data, computer code and final results. Analysis of data issued from high throughput experiments can be extremely complex, involving multiple steps from data formating, pre-processing to statistical inference. Thus it is important that all steps be recorded for full reproducibility. Unfortunatelly, this is difficult to do with a point-and-click software interface, where there is no easy to save intermediate step. This is not to mention the fact that the ``manual'' analysis of a typical high throughput data set would typically require the used of multiple software and be very time consuming. In addition, it is not clear how robust the conclusions of a study are to small perturbations in any of these analysis steps. As such, it might be a good idea to be able to quickly redo an analysis after tuning some parameters to optimize the analysis; something that is not practical with a typical point-and-click software. 


\subsection{Tools for reproducible analyses}
In the recent years, several open source, community-based projects have emerged that enable researchers to contrusct and share complete and fully reproducible data analysis pipelines.
The Bioconductor project, based on the R statistical language, provide more than 500 software packages for the analysis of a wide range of biomedical data from gene expression microarrays to flow cytometry and next generation sequencing. These packages can be combined via a script written in the R language to form complex data analysis pipelines, connect to data reporistory, and generate high quality graphics. The resulting R script can then be used to record and later reproduce the analysis (along with all input parameters). Because all steps of the analysis are automated when the script is executed, it makes it easier to assess the robustness of the results when tuning some parameters. Other similar projects with perhaps more focused capabilities include Biopython and Bioperl that are based on the Python and Perl languages, respectively. For example, neither BioPython nor Perl have tools for the analysis of flow cytometry data. 

Even though several graphical user interfaces (e.g. RStudio for R) are available for writing computer scripts based on R/Bioconductor (or BioPerl, BioPython), the learning curve can still be steep for novice users. Several web based tools are now available to construct data analysis pipelines using a combinations of available modules that are for the most part wrappers for packages available in R, Perl or Python (or some other language). For example, a popular open source graphical-user-interface for gene expression analysis, GenePattern, versions every pipeline and its methods, ensuring that each version of a pipeline (and its results) remain static, even the pipeline continues to progress. A more recent project, GenomeSpace, funded by the National Human Genome Research Institute (NHGRI) can now combine GenePattern with other popular Bioinformatics tools including Galaxy, Cytoscape and the UCSC genome browser. As such, users can perform all of their analysis using a single platform. In the clinical and immunological field, LabKey is a popular OS tool for sharing immunological data and building complex analysis pipelines that can be shared with other users. LabKey is currently being used by large research networks including the CAVD, the HIV Vaccine Trial Network and the Immune Tolerance Network.

\subsection{Standards and code sharing}
In the same fashion that experimental protocols need to be published in order for an experiment to be reproduced, computer code, software and data should also be published along with the results of a data analysis. Ideally, software would be open source and computer code would be well package and standardize to facilitate exchange and usability. Both Bioconductor and GenePattern, mentioned above, provide facilities for users to package and share code with other users. Bioconductor is based on the R packaging system, which is highly standardized and has been a driving force behind R and Bioconductor gained in popularity. Bioconductor goes even further by 1) ensuring that all submitted packages are peer reviewed and 2) by providing version control repositories and built systems where source code is maintained, versioned and binaries automatically built for all operating systems. 
Among other things, the peer review process ensures that the package follows some basic guidelines, are well documented, work as advertised and are useful to the community. The open source and versioning system provide full access to algorithms and their implementation which are crucial to obtain full reproducibility. 
For users that want to version and share software code outside of the Bioconductor (or similar) project, there exist many free web-based hosting services to store, version and share code (and even data). One of our favorite platform is GitHub, which the componay markets as "Social Coding for all". GitHub makes it easy for anyone to store and version control computer code, packages, create documents, webpages and even wikis to document their code. The social aspect of GitHub makes it easy for users to work in teams on a common project. GitHub is free of all open source projects.  

Unfortunately, very few journals have code sharing/software policies and even less have requirements on the code/software being open access. For example, BMC Bioinformatics, for which one of us is associate editor, only has policies for Software articles, and even for these the source code is not required, only an executable. PLoS One requires to release software and make code open source for submission in which software is the central part of the paper. Although this policy is clearer, it is still up to the editor/reviewers to decide whether software was a central part. In a day and age where most experiments generate large amounts of data, software is always going to play a central part, so why not make this policy universal for all submission involving data analysis?
This being said, based on our own experience as associate editors, we feel that reviewers are pushing in the right direction by asking that code be open source and released along with the paper. So even if journal have no clear policies yet, we, the community, can enforce that code be released everytime we review a paper. 

\subsection{Authoring tools}
% Sweave, knitr, Rstudio, openoffice, pandoc
% GenePattern and Word plugin

Several tools have been proposed to automatically incorporate 
reproducible data analysis pipelines or computer code into documents.
An example is the GenePattern Word plugin that can be used to embed analysis pipelines in a document and rerun them on any GenePattern server from the Word application. 
Another example that is popular among statisticians and bioinformatics is the Sweave literate language that allows one to create dynamic reports by embedding R code in latex documents. This is our preferred approach because it is open source and does not depend on propriety software. In fact, every Bioconductor package is required to have a fully reproducible documentation (called vignette) written in the Sweave language. In addition, recent software development such as RStudio and knitr made working with Sweave even more accessible, which should reduce the learning curve for most users. In fact, this article was written using the Sweave language and processed using RStudio, and the source file (along with all versions of it) is available from GitHub.
Ideally, all material including the Sweave source file, computer code and data, which Gentleman refers to as a \textit{compendium}, would be made available along with the final version of the manuscript and be open access, allowing anyone to reproduce the results or identify potential problems in the analysis. An option would be to package, code, data and the Sweave source file into an R package for ease of distribution as is commonly done for Bioconductor data packages.
Journals that promote to this openness should further improve their impact versus non open journals by giving more credibility to the published results. Unfortunately, currently very few journals are pushing for reproducibility and even less have clear reproducibility policies. An example of a journal moving in the right direction is Biostatistics, for which one of us is associate editor. Biostatistics now has a reproducility guideline and is now working with authors towards making sure that published results are reproducible given that data and code are provided (as described in the guilelines). When data and code are provided, and results are reproducible, the article is marked with an R. 


\section{Conclusion}
% Talk about the importance of bioinformatics/biostatistics and LDO groups

\end{document}